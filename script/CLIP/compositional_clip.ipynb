{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\CS4248\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPModel, CLIPTokenizer, CLIPTextModel, CLIPVisionModel, get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import PIL\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import regex\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "elco_df = pd.read_csv('../../data/ELCo.csv')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# set_seed()\n",
    "\n",
    "def comp_type_map(comp_type):\n",
    "    type_to_label = {'Direct': 0, 'Metaphorical': 1, 'Semantic list': 2, 'Reduplication': 3, 'Single': 4}\n",
    "    return type_to_label[comp_type]\n",
    "\n",
    "def label_to_comp_type(label):\n",
    "    label_to_type = {0: 'Direct', 1: 'Metaphorical', 2: 'Semantic list', 3: 'Reduplication', 4: 'Single'}\n",
    "    return label_to_type[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiCLIP(nn.Module):\n",
    "    def __init__(self, clip_model='openai/clip-vit-base-patch32'):\n",
    "        super().__init__()\n",
    "        self.vit = CLIPVisionModel.from_pretrained(clip_model)\n",
    "\n",
    "        # freeze ViT in early training\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "    def extract_embedding(self, image):\n",
    "        with torch.no_grad():\n",
    "            return self.vit(pixel_values=image).pooler_output\n",
    "    \n",
    "    def forward(self, image):\n",
    "        image_embedding = self.extract_embedding(image)\n",
    "        return image_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])):\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        image = PIL.Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.images[idx].removesuffix('.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 14/14 [00:54<00:00,  3.89s/it]\n"
     ]
    }
   ],
   "source": [
    "emoji_image_dataset = EmojiImageDataset('images')\n",
    "emoji_image_dataloader = DataLoader(emoji_image_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model = EmojiCLIP().eval().to(device)\n",
    "all_embeddings = []\n",
    "all_emoji_descs = []\n",
    "with torch.no_grad():\n",
    "    for batch_images, batch_descs in tqdm(emoji_image_dataloader, desc='Extracting embeddings'):\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_embeddings = model(batch_images)\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "        all_emoji_descs.extend(batch_descs)\n",
    "\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0) # (N, 768)\n",
    "emoji_embed_dict = {desc: embed for desc, embed in zip(all_emoji_descs, all_embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=512, num_classes=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojisDataset(Dataset):\n",
    "    def __init__(self, emoji_embed_dict, elco_df, em_max_len=3, text_max_len=4, tokenizer='openai/clip-vit-base-patch32'):\n",
    "        super().__init__()\n",
    "        self.elco_df = elco_df\n",
    "        self.emoji_descriptions = [self.preprocess_emoji_description(desc) for desc in elco_df[\"Description\"]]\n",
    "        self.raw_emoji_descriptions = elco_df[\"Description\"].values\n",
    "        self.emoji_embed_dict = emoji_embed_dict\n",
    "        self.em_max_len = em_max_len\n",
    "        self.text_max_len = text_max_len\n",
    "        self.clip_tokenizer = CLIPTokenizer.from_pretrained(tokenizer)\n",
    "        self.composition_type = [comp_type_map(t) for t in elco_df['Composition strategy'].values]\n",
    "    \n",
    "    def preprocess_emoji_description(self, text):\n",
    "        text = text.replace('\\'\\'', '').lower()\n",
    "        split_text = regex.findall(r'\\':?(.*?):?\\'', text)\n",
    "        return split_text\n",
    "    def preprocess_en(self, text):\n",
    "        return text.lower().strip()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.emoji_descriptions)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        emoji_descs = self.emoji_descriptions[index]\n",
    "        emoji_embeds = [self.emoji_embed_dict[desc] for desc in emoji_descs]\n",
    "        en_text = self.preprocess_en(self.elco_df['EN'].values[index])\n",
    "        composition_type = self.composition_type[index]\n",
    "        assert len(emoji_embeds) == len(emoji_descs)\n",
    "        if len(emoji_embeds) < self.em_max_len:\n",
    "            emoji_embeds += [torch.zeros_like(emoji_embeds[0]) for _ in range(self.em_max_len - len(emoji_embeds))]\n",
    "        else:\n",
    "            emoji_embeds = emoji_embeds[:self.em_max_len]\n",
    "        # emoji_embeds = torch.stack(emoji_embeds, dim=0)\n",
    "        emoji_embeds = torch.concatenate(emoji_embeds, dim=-1)\n",
    "        \n",
    "        en_tokens = self.clip_tokenizer(en_text, \n",
    "                                  truncation=True, \n",
    "                                  padding='max_length',\n",
    "                                  return_tensors='pt',\n",
    "                                  max_length=self.text_max_len)\n",
    "        en_tokens = {k: v.squeeze(0) for k, v in en_tokens.items()}\n",
    "        return (emoji_embeds, en_tokens), composition_type\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator()\n",
    "# generator.manual_seed(42)\n",
    "train_df, validate_df = train_test_split(elco_df, test_size=0.1, random_state=42)\n",
    "# validate_df, test_df = train_test_split(validate_df, test_size=0.3, random_state=42)\n",
    "emojis_train_dataset = EmojisDataset(emoji_embed_dict, train_df)\n",
    "emojis_train_dataloader = DataLoader(emojis_train_dataset, batch_size=32, shuffle=True, drop_last=True, generator=generator)\n",
    "emojis_validate_dataset = EmojisDataset(emoji_embed_dict, validate_df)\n",
    "emojis_validate_dataloader = DataLoader(emojis_validate_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
    "# emojis_test_dataset = EmojisDataset(emoji_embed_dict, test_df)\n",
    "# emojis_test_dataloader = DataLoader(emojis_test_dataset, batch_size=32, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2304]) torch.Size([32, 4])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for em_en_pair, ctype in emojis_train_dataloader:\n",
    "    print(em_en_pair[0].shape, em_en_pair[1]['input_ids'].shape)\n",
    "    print(ctype[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiCompositionModel(nn.Module):\n",
    "    def __init__(self, text_encoder='openai/clip-vit-base-patch32',\n",
    "                 image_embed_dim=768, projection_dim=512, num_layers=2, freeze_clip=False):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # self.emoji_transformer = ImageEmbedTransformer(embedding_dim=image_embed_dim, num_layers=num_layers)\n",
    "\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(text_encoder).text_model\n",
    "        self.text_hidden_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        if freeze_clip:\n",
    "            for name, param in self.text_encoder.named_parameters():\n",
    "                    # if \"encoder.layers.10\" in name or \"encoder.layers.11\" in name:\n",
    "                    #     param.requires_grad = True\n",
    "                    # else:\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "        \n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(self.text_hidden_dim, projection_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(projection_dim)\n",
    "            )\n",
    "        self.image_proj = nn.Sequential(\n",
    "            nn.Linear(image_embed_dim, projection_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(projection_dim)\n",
    "            )\n",
    "        \n",
    "        self.classifier = TypeClassifier(input_dim=projection_dim * 4, hidden_dim=projection_dim * 2, num_classes=5)\n",
    "        \n",
    "    def forward(self, img_seq, text_input):\n",
    "        # z_image = self.emoji_transformer(img_seq)[:, 0, :]\n",
    "        z_image = self.image_proj(img_seq)\n",
    "        text_out = self.text_encoder(**text_input).pooler_output\n",
    "        z_text = self.text_proj(text_out)\n",
    "\n",
    "        z_image = F.normalize(z_image, dim=-1)\n",
    "        z_text = F.normalize(z_text, dim=-1)\n",
    "        \n",
    "        z_concat = torch.cat([z_image, z_text, torch.abs(z_image - z_text), z_image*z_text], dim=-1)\n",
    "        output = self.classifier(z_concat)\n",
    "\n",
    "        return (z_image, z_text), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, accuracy):\n",
    "        if accuracy > 0.7:\n",
    "            self.early_stop = True\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yyxxc\\AppData\\Local\\Temp\\ipykernel_2992\\443105275.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  classifier_loss = criterion(output, torch.tensor(ctype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 5.015411843424258, Accuracy: 0.43373493975903615\n",
      "Epoch 1 - Loss: 4.861016470453014, Accuracy: 0.43373493975903615\n",
      "Epoch 2 - Loss: 4.65356069025786, Accuracy: 0.43373493975903615\n",
      "Epoch 3 - Loss: 4.537453889846802, Accuracy: 0.43373493975903615\n",
      "Epoch 4 - Loss: 4.4455646950265635, Accuracy: 0.4578313253012048\n",
      "Epoch 5 - Loss: 4.356900650521983, Accuracy: 0.5240963855421686\n",
      "Epoch 6 - Loss: 4.270261961480846, Accuracy: 0.5421686746987951\n",
      "Epoch 7 - Loss: 4.172721396321836, Accuracy: 0.5903614457831325\n",
      "Epoch 8 - Loss: 4.089020371437073, Accuracy: 0.6204819277108434\n",
      "Epoch 9 - Loss: 4.004605225894762, Accuracy: 0.6445783132530121\n",
      "Epoch 10 - Loss: 3.922036824019059, Accuracy: 0.6144578313253012\n",
      "Epoch 11 - Loss: 3.8473214273867398, Accuracy: 0.6385542168674698\n",
      "Epoch 12 - Loss: 3.7773318342540576, Accuracy: 0.6265060240963856\n",
      "Epoch 13 - Loss: 3.716186487156412, Accuracy: 0.6445783132530121\n",
      "Epoch 14 - Loss: 3.645752435145171, Accuracy: 0.6506024096385542\n",
      "Epoch 15 - Loss: 3.5882493361182837, Accuracy: 0.6506024096385542\n",
      "Epoch 16 - Loss: 3.5337802016216777, Accuracy: 0.6445783132530121\n",
      "Epoch 17 - Loss: 3.4828107408855273, Accuracy: 0.6506024096385542\n",
      "Epoch 18 - Loss: 3.439980045608852, Accuracy: 0.6626506024096386\n",
      "Epoch 19 - Loss: 3.3896287835162617, Accuracy: 0.6506024096385542\n",
      "Epoch 20 - Loss: 3.3500368543293164, Accuracy: 0.6686746987951807\n",
      "Epoch 21 - Loss: 3.3149041559385215, Accuracy: 0.6807228915662651\n",
      "Epoch 22 - Loss: 3.2875540982122007, Accuracy: 0.6506024096385542\n",
      "Epoch 23 - Loss: 3.2511661363684614, Accuracy: 0.6867469879518072\n",
      "Epoch 24 - Loss: 3.226725557576055, Accuracy: 0.6927710843373494\n",
      "Epoch 25 - Loss: 3.2037939455198203, Accuracy: 0.6686746987951807\n",
      "Epoch 26 - Loss: 3.177752059438954, Accuracy: 0.6626506024096386\n",
      "Epoch 27 - Loss: 3.161094131677047, Accuracy: 0.6746987951807228\n",
      "Epoch 28 - Loss: 3.1380012294520503, Accuracy: 0.6746987951807228\n",
      "Epoch 29 - Loss: 3.1200354254764058, Accuracy: 0.6927710843373494\n",
      "Epoch 30 - Loss: 3.1051683322243067, Accuracy: 0.6746987951807228\n",
      "Epoch 31 - Loss: 3.086544052414272, Accuracy: 0.6746987951807228\n",
      "Epoch 32 - Loss: 3.0704369700473286, Accuracy: 0.6626506024096386\n",
      "Epoch 33 - Loss: 3.057475981505021, Accuracy: 0.6746987951807228\n",
      "Epoch 34 - Loss: 3.038514168366142, Accuracy: 0.6867469879518072\n",
      "Epoch 35 - Loss: 3.025926393011342, Accuracy: 0.6746987951807228\n",
      "Epoch 36 - Loss: 3.011383295059204, Accuracy: 0.6867469879518072\n",
      "Epoch 37 - Loss: 2.9996536140856533, Accuracy: 0.6867469879518072\n",
      "Epoch 38 - Loss: 2.9874500295390254, Accuracy: 0.6867469879518072\n",
      "Epoch 39 - Loss: 2.9730077463647593, Accuracy: 0.6807228915662651\n",
      "Epoch 40 - Loss: 2.9626832163852193, Accuracy: 0.6867469879518072\n",
      "Epoch 41 - Loss: 2.9490010945693306, Accuracy: 0.7048192771084337\n",
      "Early stopping at epoch 41\n"
     ]
    }
   ],
   "source": [
    "# reconstruct text dataloader -> each bacth is a list of all possible en for the emojis in given batch\n",
    "en_em_model = EmojiCompositionModel(image_embed_dim=2304, freeze_clip=True).to(device)\n",
    "optimizer = torch.optim.AdamW(en_em_model.parameters(), lr=2e-5)\n",
    "early_stopping = EarlyStopping(patience=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 70\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    en_em_model.train()\n",
    "    for (emoji_embeds, en_input), ctype in emojis_train_dataloader:\n",
    "        (z_emojis, z_text), output = en_em_model(emoji_embeds, en_input)\n",
    "        \n",
    "        classifier_loss = criterion(output, torch.tensor(ctype))\n",
    "        \n",
    "        match_labels = torch.arange(emojis_train_dataloader.batch_size).to(device)\n",
    "        loss_per_emojis = z_emojis @ z_text.T\n",
    "        loss_per_text = loss_per_emojis.T\n",
    "        loss_em = F.cross_entropy(loss_per_emojis, match_labels)\n",
    "        loss_text = F.cross_entropy(loss_per_text, match_labels)\n",
    "\n",
    "        loss = classifier_loss + (loss_em + loss_text) / 2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(emojis_train_dataloader)\n",
    "        \n",
    "    en_em_model.eval()\n",
    "    correct_count = 0\n",
    "    with torch.no_grad():\n",
    "        for (emoji_embeds, en_input), ctype in emojis_validate_dataloader:\n",
    "            _, output = en_em_model(emoji_embeds, en_input)\n",
    "            prediction = torch.argmax(output, dim=-1)\n",
    "            correct_count += torch.sum(prediction == ctype).item()\n",
    "        en_accuracy = correct_count / len(emojis_validate_dataset)\n",
    "    \n",
    "    print(f'Epoch {epoch} - Loss: {total_loss}, Accuracy: {en_accuracy}')\n",
    "    \n",
    "    early_stopping(total_loss, en_accuracy)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
